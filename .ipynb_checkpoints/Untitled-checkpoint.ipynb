{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Summary:     A collections of functions to generate features.\n",
    "Description:\n",
    "Author:      Kunyu He, CAPP'20\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------#\n",
    "INPUT_DIR = \"./data/\"\n",
    "OUTPUT_DIR = \"./processed_data/\"\n",
    "LOG_DIR = \"./logs/featureEngineering/\"\n",
    "\n",
    "TRAIN_FILE = \"train.csv\"\n",
    "TEST_FILE = \"test.csv\"\n",
    "\n",
    "SCALERS = [StandardScaler, MinMaxScaler]\n",
    "\n",
    "# logging\n",
    "logger= logging.getLogger('featureEngineering')\n",
    "logger.setLevel(logging.INFO)\n",
    "ch = logging.StreamHandler()\n",
    "logger.addHandler(ch)\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------#\n",
    "def read_data(file_name, drop_na=False):\n",
    "    \"\"\"\n",
    "    Read credit data in the .csv file and data types from the .json file.\n",
    "\n",
    "    Inputs:\n",
    "        - data_file (string): name of the data file.\n",
    "        - drop_na (bool): whether to drop rows with any missing values\n",
    "\n",
    "    Returns:\n",
    "        (DataFrame) clean data set with correct data types\n",
    "\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(INPUT_DIR + file_name)\n",
    "\n",
    "    if drop_na:\n",
    "        data.dropna(axis=0, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def ask():\n",
    "    \"\"\"\n",
    "    Ask user for choice of an imputer and a scaler.\n",
    "\n",
    "    \"\"\"\n",
    "    scaler_index = int(input((\"Up till now we support:\\n\"\n",
    "                              \"\\t1. StandardScaler\\n\"\n",
    "                              \"\\t2. MinMaxScaler\\n\"\n",
    "                              \"Please input a scaler index (1 or 2):\\n\")))\n",
    "\n",
    "    return scaler_index\n",
    "\n",
    "\n",
    "class FeaturePipeLine:\n",
    "    \"\"\"\n",
    "    Preprocess pipeline for a data set from CSV file. Modify the class\n",
    "    variables to fill in missing values, combine multinomial variables to ones\n",
    "    with less levels and binaries, and apply one-hot-encoding. Then split data\n",
    "    into features and traget, drop rows with missing labels and some columns.\n",
    "    At last, apply scaling.\n",
    "\n",
    "    \"\"\"\n",
    "    TO_DESCRETIZE = {'Age': 5}\n",
    "    RIGHT_INCLUSIVE = {'Age': True}\n",
    "\n",
    "    TO_FILL_NA = {'Cabin': \"None\",\n",
    "                  'Embarked': \"Unknown\"}\n",
    "\n",
    "    TO_COMBINE = {}\n",
    "    TO_BINARIES = {'Sex': 'auto',\n",
    "                   'Cabin': 'auto'}\n",
    "    TO_ONE_HOT = {'PClass', 'Embarked', 'Age'}\n",
    "\n",
    "    TARGET = 'Survived'\n",
    "    TO_DROP = ['PassengerId', 'Ticket', 'Name']\n",
    "\n",
    "    SCALERS = [StandardScaler, MinMaxScaler]\n",
    "    SCALER_NAMES = [\"Standard Scaler\", \"MinMax Scaler\"]\n",
    "\n",
    "    def __init__(self, file_name, ask_user=True, verbose=True, drop_na=False):\n",
    "        \"\"\"\n",
    "        Construct a preprocessing pipeline given name of the data file.\n",
    "\n",
    "        Inputs:\n",
    "            - file_name (string): name of the data file\n",
    "            - verbose (bool): whether to make extended printing in\n",
    "                preprocessing\n",
    "            - drop_na (bool): whether to drop rows with missing values\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info((\"**-----------------------------------------------**\\n\"))\n",
    "        logger.info(\"Creating the preprocessing pipeline for '{}'.\".format(\\\n",
    "            file_name))\n",
    "        self.data = read_data(file_name, drop_na)\n",
    "        self.verbose = verbose\n",
    "        logger.info(\"Finished reading cleaned data.\")\n",
    "\n",
    "        if ask_user:\n",
    "            self.scaler_index = ask()\n",
    "        else:\n",
    "            self.scaler_index = 1\n",
    "        logger.info(\"Pipeline using scaler {}\".\\\n",
    "                    format(self.SCALER_NAMES[self.scaler_index - 1]))\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def discretize(self):\n",
    "        \"\"\"\n",
    "        Discretizes continuous variables into multinomials.\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info((\"\\n\\n**-------------------------------------------**\\n\"))\n",
    "        logger.info(\"Start to discretizes continuous variables:\")\n",
    "\n",
    "        for var, n in self.TO_DESCRETIZE.items():\n",
    "            self.data[var] = pd.cut(self.data[var], n,\n",
    "                                    right=self.RIGHT_INCLUSIVE[var]).cat.codes\n",
    "\n",
    "            if self.verbose:\n",
    "                if not self.data[var].isnull().sum():\n",
    "                    logger.info((\"\\tThere are missing values in '{}', \"\n",
    "                        \"discretized it into {} bins, where '-1' indicates \"\n",
    "                        \"that the value is missing.\".format(var, n + 1)))\n",
    "                else:\n",
    "                    logger.info(\"\\tDiscretized '{}' into {} bins.\".\\\n",
    "                                format(var, n))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fill_na(self):\n",
    "        \"\"\"\n",
    "        Fill in missing data with desired entry.\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info((\"\\n\\n**-------------------------------------------**\\n\"))\n",
    "        logger.info(\"Start to fill in missing values:\")\n",
    "\n",
    "        for var, fill in self.TO_FILL_NA.items():\n",
    "            self.data[var].fillna(value=fill, inplace=True)\n",
    "\n",
    "            if self.verbose:\n",
    "                logger.info(\"\\tFilled missing values in '{}' with '{}'.\".\\\n",
    "                      format(var, fill))\n",
    "\n",
    "            if fill == \"None\":\n",
    "                to_combine = [col for col in list(self.data[var].unique())\n",
    "                              if col != \"None\"]\n",
    "                self.TO_COMBINE[var] = {\"Yes\": to_combine}\n",
    "                logger.info(\"\\t\\t'{}' added to 'TO_COMBINE'\".format(var))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def to_combine(self):\n",
    "        \"\"\"\n",
    "        Combine some unecessary levels of multinomials.\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info((\"\\n\\n**-------------------------------------------**\\n\"))\n",
    "        logger.info(\"Start to combine unnecessary levels of multinomials.\")\n",
    "\n",
    "        for var, dict_combine in self.TO_COMBINE.items():\n",
    "            for combined, lst_combine in dict_combine.items():\n",
    "                self.data.loc[self.data[var].isin(lst_combine), var] = combined\n",
    "\n",
    "            if self.verbose:\n",
    "                logger.info(\"\\tCombinations of levels on '{}'.\".format(var))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def to_binary(self):\n",
    "        \"\"\"\n",
    "        Trasform variables to binaries.\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info((\"\\n\\n**-------------------------------------------**\\n\"))\n",
    "        logger.info((\"Start to transform the following variables: {} to \"\n",
    "                     \"Binaries.\").format(list(self.TO_BINARIES.keys())))\n",
    "\n",
    "        for var, cats in self.TO_BINARIES.items():\n",
    "            enc = OrdinalEncoder(categories=cats)\n",
    "            self.data[var] = enc.fit_transform(np.array(self.data[var]).\\\n",
    "                                               reshape(-1, 1))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def one_hot(self):\n",
    "        \"\"\"\n",
    "        Ccreates binary/dummy variables from multinomials, drops the original\n",
    "        and inserts the dummies back.\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info((\"\\n\\n**-------------------------------------------**\\n\"))\n",
    "        logger.info((\"Start to apply one-hot-encoding to the following \"\n",
    "                     \"categorical variables: {}\\n\").format(self.TO_ONE_HOT))\n",
    "\n",
    "        for var in self.TO_ONE_HOT:\n",
    "            dummies = pd.get_dummies(self.data[var], prefix=var)\n",
    "            self.data.drop(var, axis=1, inplace=True)\n",
    "            self.data = pd.concat([self.data, dummies], axis=1)\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "**-----------------------------------------------**\n",
      "\n",
      "Creating the preprocessing pipeline for 'train.csv'.\n",
      "Finished reading cleaned data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up till now we support:\n",
      "\t1. StandardScaler\n",
      "\t2. MinMaxScaler\n",
      "Please input a scaler index (1 or 2):\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipeline using scaler Standard Scaler\n",
      "\n",
      "\n",
      "**-------------------------------------------**\n",
      "\n",
      "Start to discretizes continuous variables:\n",
      "\tThere are missing values in 'Age', discretized it into 6 bins, where '-1' indicates that the value is missing.\n",
      "\n",
      "\n",
      "**-------------------------------------------**\n",
      "\n",
      "Start to fill in missing values:\n",
      "\tFilled missing values in 'Cabin' with 'None'.\n",
      "\t\t'Cabin' added to 'TO_COMBINE'\n",
      "\tFilled missing values in 'Embarked' with 'Unknown'.\n",
      "\n",
      "\n",
      "**-------------------------------------------**\n",
      "\n",
      "Start to combine unnecessary levels of multinomials.\n",
      "\tCombinations of levels on 'Cabin'.\n",
      "\n",
      "\n",
      "**-------------------------------------------**\n",
      "\n",
      "Start to transform the following variables: ['Sex', 'Cabin'] to Binaries.\n",
      "\n",
      "\n",
      "**-------------------------------------------**\n",
      "\n",
      "Start to apply one-hot-encoding to the following categorical variables: {'PClass', 'Age', 'Embarked'}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'PClass'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\envs\\intel\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2655\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2656\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2657\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'PClass'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5c17eded4b8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeaturePipeLine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTRAIN_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscretize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_na\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_combine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-626870f8e382>\u001b[0m in \u001b[0;36mone_hot\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTO_ONE_HOT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m             \u001b[0mdummies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummies\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\intel\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\intel\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2658\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2659\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'PClass'"
     ]
    }
   ],
   "source": [
    "pipe = FeaturePipeLine(TRAIN_FILE, ask_user=False)\n",
    "pipe.discretize().fill_na().to_combine().to_binary().one_hot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex  Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris  1.0    1      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  0.0    2      1      0   \n",
       "2                             Heikkinen, Miss. Laina  0.0    1      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  0.0    2      1      0   \n",
       "4                           Allen, Mr. William Henry  1.0    2      0      0   \n",
       "\n",
       "             Ticket     Fare  Cabin Embarked  \n",
       "0         A/5 21171   7.2500    0.0        S  \n",
       "1          PC 17599  71.2833    1.0        C  \n",
       "2  STON/O2. 3101282   7.9250    0.0        S  \n",
       "3            113803  53.1000    1.0        S  \n",
       "4            373450   8.0500    0.0        S  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    346\n",
       " 2    188\n",
       "-1    177\n",
       " 0    100\n",
       " 3     69\n",
       " 4     11\n",
       "Name: Age, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.data.Age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           0\n",
       "Cabin          0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
